|V|=256, |E|=480
graph type: grid
Filename: /home/cloud-user/code/source_finding/ic.py

Line #    Mem usage    Increment   Line Contents
================================================
    94   85.590 MiB    0.000 MiB   @profile
    95                             def infection_time_estimation(g, n_rounds, return_node2id=False):
    96                                 """
    97                                 estimate the infection time distribution
    98                                 n_rounds of cascades for *each* node
    99                             
   100                                 Returns:
   101                                 dict of source to 2D sparse matrices (node to infection time probabilities)
   102                                     can be viewed as 3D tensor:
   103                                     N x N x max(t), source to node to infection time
   104                                     dict source to nodes' infection time distribution
   105                                 """
   106   85.590 MiB    0.000 MiB       node2id = {n: i for i, n in enumerate(g.nodes_iter())}
   107                             
   108                                 if True:
   109                                     # parallel and pandas.Dataframe approach
   110                                     # faster but more memory consuming
   111   86.438 MiB    0.848 MiB           tmp_file_paths = Parallel(n_jobs=-1)(delayed(run_one_round)(sample_graph_from_infection(g), node2id)
   112   86.738 MiB    0.301 MiB                                                for i in range(n_rounds))
   113   91.258 MiB    4.520 MiB           dfs = [pd.read_csv(p, sep=',',
   114                                                        names=['source', 'node', 'time'],
   115                                                        dtype=np.uint16)
   116   91.258 MiB    0.000 MiB                  for p in tmp_file_paths]
   117  101.367 MiB   10.109 MiB           df = pd.concat(dfs, axis=0)
   118                             
   119                                     # remove files
   120  101.367 MiB    0.000 MiB           for p in tmp_file_paths:
   121  101.367 MiB    0.000 MiB               os.unlink(p)
   122                             
   123  102.809 MiB    1.441 MiB           n_times = df['time'].max() + 2
   124  102.809 MiB    0.000 MiB           d = {}
   125                             
   126  127.066 MiB   24.258 MiB           for s, sdf in tqdm(df.groupby('source')):
   127  127.066 MiB    0.000 MiB               nt_series = pd.Series([(n, t) for n, t in zip(sdf['node'], sdf['time'])])
   128  127.066 MiB    0.000 MiB               counts = nt_series.value_counts()
   129                                     
   130  127.066 MiB    0.000 MiB               row = [r for r, _ in counts.index]
   131  127.066 MiB    0.000 MiB               col = [c for _, c in counts.index]
   132                                         # row, col = zip(*counts.index.tolist())
   133                             
   134  127.066 MiB    0.000 MiB               row = np.array(row)
   135  127.066 MiB    0.000 MiB               col = np.array(col)
   136                                         
   137                                         # get uninfected counts
   138  127.066 MiB    0.000 MiB               sum_df = pd.DataFrame.from_dict({'r': row, 'c': counts.as_matrix()})
   139  127.066 MiB    0.000 MiB               row_sums = sum_df.groupby('r')['c'].sum()
   140                             
   141  127.066 MiB    0.000 MiB               uninfected_counts = (np.ones(g.number_of_nodes(), dtype=np.float) * n_rounds -
   142  127.066 MiB    0.000 MiB                                    row_sums.as_matrix())
   143  127.066 MiB    0.000 MiB               data = np.concatenate((counts.as_matrix(),
   144  127.066 MiB    0.000 MiB                                      uninfected_counts))
   145  127.066 MiB    0.000 MiB               data /= n_rounds
   146  127.066 MiB    0.000 MiB               row = np.concatenate((row,
   147  127.066 MiB    0.000 MiB                                     np.arange(g.number_of_nodes())))
   148  127.066 MiB    0.000 MiB               col = np.concatenate((col,
   149  127.066 MiB    0.000 MiB                                     (n_times-1) * np.ones(g.number_of_nodes())))
   150                                         
   151  127.066 MiB    0.000 MiB               d[s] = csr_matrix((data, (row, col)),
   152  127.066 MiB    0.000 MiB                                 shape=(g.number_of_nodes(), n_times))
   153                                 else:
   154                                     # vanilla approach
   155                                     # might spend less memory
   156                                     s2n_times_counter = defaultdict(lambda: defaultdict(int))
   157                             
   158                                     inf = float('inf')
   159                                     # run in serial to save memory
   160                                     for i in tqdm(range(n_rounds)):
   161                                         sampled_g = sample_graph_from_infection(g)
   162                                         # this is using Dijkstra
   163                                         # s2t_len = nx.shortest_path_length(sampled_g, weight='d')
   164                             
   165                                         # this is serial BFS
   166                                         s2t_len = nx.shortest_path_length(sampled_g)
   167                             
   168                                         # this is parallel BFS (slow)
   169                                         # results = Parallel(n_jobs=-1)(
   170                                         #     delayed(nx.single_source_shortest_path_length)(g, n)
   171                                         #     for n in g.nodes_iter())
   172                                         # s2t_len = {n: t_len for n, t_len in zip(g.nodes_iter(), results)}
   173                                         
   174                                         for s in s2t_len:  # one cascade for each node
   175                                             for n in sampled_g.nodes_iter():
   176                                                 s2n_times_counter[(s, n)][s2t_len[s].get(n, inf)] += 1
   177                             
   178                                     all_times = np.array([t
   179                                                           for times_counter in s2n_times_counter.values()
   180                                                           for t in times_counter.keys()])
   181                                     all_times = np.ravel(all_times)
   182                                     unique_values = np.unique(all_times)
   183                             
   184                                     min_val, max_val = (int(unique_values.min()),
   185                                                         int(unique_values[np.invert(np.isinf(unique_values))].max()))
   186                                     n_times = max_val - min_val + 2
   187                                     # using dict to 2D sparse matrix because there is no 3D sparse matrix
   188                                     d = dict()
   189                                     for s in tqdm(g.nodes_iter()):
   190                                         i = node2id[s]
   191                                         row = []  # infected node
   192                                         col = []  # infection time
   193                                         data = []  # probabilities
   194                                         for n in g.nodes_iter():
   195                                             j = node2id[n]
   196                                             cnts = s2n_times_counter[(s, n)]
   197                                             cnts[n_times-1] = cnts[float('inf')]
   198                                             del cnts[float('inf')]
   199                                             row += [j] * len(cnts)
   200                                             col_slice, cnts_list = map(list, zip(*cnts.items()))
   201                                             col += col_slice
   202                                             cnts_array = np.array(cnts_list, dtype=np.float)
   203                                             cnts_array /= cnts_array.sum()
   204                                             data += cnts_array.tolist()
   205                             
   206                                         d[i] = csr_matrix((data, (row, col)), shape=(g.number_of_nodes(), n_times))
   207                             
   208  127.066 MiB    0.000 MiB       if return_node2id:
   209  127.066 MiB    0.000 MiB           return d, node2id
   210                                 else:
   211                                     return d


